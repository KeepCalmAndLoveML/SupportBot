{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nИнтересные kernels\\nhttps://www.kaggle.com/hubert0527/spacy-name-entity-recognition\\nhttps://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Интересные kernels\n",
    "https://www.kaggle.com/hubert0527/spacy-name-entity-recognition\n",
    "https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137835\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When do you use г‚· instead of гЃ—?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     question1  \\\n",
       "0               How can I be a good geologist?   \n",
       "1          When do you use г‚· instead of гЃ—?   \n",
       "2  How do I read and find my YouTube comments?   \n",
       "3         What can make Physics easy to learn?   \n",
       "4  What was your first sexual experience like?   \n",
       "\n",
       "                                   question2  is_duplicate  \n",
       "0  What should I do to be a great geologist?             1  \n",
       "1      When do you use \"&\" instead of \"and\"?             0  \n",
       "2     How can I see all my Youtube comments?             1  \n",
       "3    How can you make physics easy to learn?             1  \n",
       "4     What was your first sexual experience?             1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "folder = r'C:\\TMP'\n",
    "\n",
    "raw_train = pd.read_csv(folder + r'\\train.csv', engine = 'python')\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "qs = [list(zip([row['question1']], [row['question2']], [row['is_duplicate']]))\n",
    "       for index, row in raw_train.iterrows() if len(str(row['question1'])) <= 50 and len(str(row['question2'])) <= 50]\n",
    "print(len(qs))\n",
    "\n",
    "tmp['question1'] = [item[0][0] for item in qs]\n",
    "tmp['question2'] = [item[0][1] for item in qs]\n",
    "tmp['is_duplicate'] = [item[0][2] for item in qs]\n",
    "raw_train = tmp\n",
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_words = [w for w in q1words.keys() if w in q2words]\n",
    "    res = 2 * len(shared_words) / (len(q1words) + len(q2words))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def get_weight(count, total_count, min_count=3):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return count / total_count\n",
    "    \n",
    "def tokenize(sentence):\n",
    "    text = re.sub(\"\\'s\", \" \", sentence) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text).lower().split()\n",
    "    return text\n",
    "    \n",
    "words = raw_train['question1'].apply(str).tolist() + raw_train['question2'].apply(str).tolist()\n",
    "words = [tokenize(s) for s in words]\n",
    "words = [word for sentence in words for word in sentence]\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count, len(words)) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec as w2v\n",
    "\n",
    "my_w2v = w2v.Word2Vec.load(folder + r'\\w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in tokenize(str(row['question1'])):\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in tokenize(str(row['question2'])):\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    similarities = [max([my_w2v.similarity(w1, w2) for w2 in q2words.keys()]) for w1 in q1words.keys()]\n",
    "    return np.sum(similarities) / len(q1words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in tokenize(str(row['question1'])):\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in tokenize(str(row['question2'])):\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    res = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_synonym(word, synonym):\n",
    "    synsets = wn.synsets(word)\n",
    "    res = []\n",
    "    for ss in synsets:\n",
    "        if ss.name().startswith(word):\n",
    "            res.append(ss.lemma_names())\n",
    "            \n",
    "    res = list(set([item for lst in res for item in lst]))\n",
    "    return synonym in res\n",
    "\n",
    "def synonym_wms(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_words = [w for w in q1words.keys() for w2 in q2words.keys() if is_synonym(w2, w) or w == w2]\n",
    "    return 2 * len(shared_words) / (len(q1words) + len(q2words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def get_prop_nouns(sentence):\n",
    "    tagged_sent = pos_tag(sentence.split())\n",
    "    proper_nouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "    \n",
    "    return proper_nouns\n",
    "\n",
    "def propn_wms(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prop1 = get_prop_nouns(' '.join(list(q1words.keys())))\n",
    "    prop2 = get_prop_nouns(' '.join(list(q2words.keys())))\n",
    "    shared_props = [w for w in prop1 if w in prop2]\n",
    "    \n",
    "    return (2 * len(shared_props)) / (len(q1words.keys()) + len(q2words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def my_max(lst):\n",
    "    if len(lst) > 0:\n",
    "        return max(lst)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def sentence_similarity(row, rec = False):\n",
    "    # Tokenize and tag\n",
    "    if not rec:\n",
    "        s1 = row['question1']\n",
    "        s2 = row['question2']\n",
    "    else:\n",
    "        s1 = row['question2']\n",
    "        s2 = row['question1']\n",
    "        \n",
    "    s1 = tokenize(str(s1))\n",
    "    s2 = tokenize(str(s2))\n",
    "    s1 = [w for w in s1 if w not in stops]\n",
    "    s2 = [w for w in s2 if w not in stops]\n",
    "        \n",
    "    sentence1 = pos_tag(s1)\n",
    "    sentence2 = pos_tag(s2)\n",
    "    \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss is not None]\n",
    "    synsets2 = [ss for ss in synsets2 if ss is not None]\n",
    "    #print(synsets1, synsets2)\n",
    "\n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = my_max([synset.path_similarity(ss) for ss in synsets2 if synset.path_similarity(ss) is not None])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "            \n",
    "    # Average the values\n",
    "    if count > 0:\n",
    "        score /= count\n",
    "    else:\n",
    "        score = 0\n",
    "    # sentence_sim(s1, s2) != sentence_sim(s2, s1), по этому берем среднее арифметическое\n",
    "    if not rec:\n",
    "        score += sentence_similarity(row, rec=True)\n",
    "        score/=2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#prop_wms = raw_train.apply(propn_wms, axis=1, raw=True)\n",
    "#weight_wms = raw_train.apply(weight_word_match_share, axis=1, raw=True)\n",
    "wms = raw_train.apply(word_match_share, axis=1, raw=True)\n",
    "weight_wms = raw_train.apply(weight_word_match_share, axis=1, raw=True)\n",
    "\n",
    "x = pd.DataFrame()\n",
    "x['wms'] = wms\n",
    "x['weight_wms'] = weight_wms\n",
    "\n",
    "y = raw_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.608898\ttrain-auc:0.737409\tvalid-logloss:0.609337\tvalid-auc:0.737179\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 50 rounds.\n",
      "[25]\ttrain-logloss:0.546017\ttrain-auc:0.756257\tvalid-logloss:0.551125\tvalid-auc:0.745855\n",
      "[50]\ttrain-logloss:0.541564\ttrain-auc:0.764909\tvalid-logloss:0.551413\tvalid-auc:0.747046\n",
      "[75]\ttrain-logloss:0.537898\ttrain-auc:0.771627\tvalid-logloss:0.551696\tvalid-auc:0.74677\n",
      "Stopping. Best iteration:\n",
      "[45]\ttrain-logloss:0.542424\ttrain-auc:0.763061\tvalid-logloss:0.551229\tvalid-auc:0.747323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_check, y_train, y_check = train_test_split(x, y, test_size=0.2, random_state=271828)\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = ['logloss', 'auc']\n",
    "params['eta'] = 0.5\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_check = xgb.DMatrix(x_check, label=y_check)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_check, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=50, verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bst.save_model(r'C:\\Python\\MODELS\\wms_weightwms.model')\n",
    "#bst.save_model(r'C:\\Python\\MODELS\\synwms_weightwms.model')\n",
    "#bst.save_model(r'C:\\Python\\MODELS\\prop_weigth_wms.model')\n",
    "#bst.save_model(r'C:\\Python\\Models\\wms_sim.model')\n",
    "#bst.save_model(r'C:\\Python\\Models\\wms_newsim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
